{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def get_key1(wordlist,val):\n",
    "    out = [k for k,v in wordlist.items() if v==val]\n",
    "    return (''.join(out))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class numpyNN:\n",
    "    def __init__(self, data, word_dim, hidden_dim, bptt_truncate=4):\n",
    "                \n",
    "        data = [\"%s %s %s\" %('start_token', x.lower(), 'end_token') for x in data]\n",
    "        data = [ (sent.replace(' ',',')) for sent in data]\n",
    "        data = [ sent.split(',') for sent in data]\n",
    "        self.data = data\n",
    "        \n",
    "        \n",
    "        all_words = [w for sent in self.data for w in sent]\n",
    "        self.u_words = set(all_words)\n",
    "        self.word_dict = dict([(w,i) for i,w in enumerate(self.u_words)])\n",
    "        data_tokens = []\n",
    "        for sent in self.data:\n",
    "            data_tokens.append(np.array([self.word_dict[w] for w in sent]))\n",
    "        self.data_tokens = data_tokens\n",
    "        \n",
    "        self.train_x = np.array(self.data_tokens[:-1])\n",
    "        self.train_y = np.array(self.data_tokens[1:])\n",
    "        \n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        self.u = np.random.randn(self.hidden_dim,self.word_dim)\n",
    "        self.v = np.random.randn(self.word_dim, self.hidden_dim)\n",
    "        self.w = np.random.randn(self.hidden_dim, self.hidden_dim)\n",
    "    \n",
    "    \n",
    "    def get_key(self,val):\n",
    "        out = [k for k,v in self.word_dict.items() if v==val]\n",
    "        return (''.join(out))  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_size =100\n",
    "unknown_token =\"U_Token\"\n",
    "#start_token =\"SENTENCE_START\"\n",
    "#end_token =\"SENTENCE_END\"\n",
    "\n",
    "sentences = []\n",
    "with open('./own_dataset.txt') as f:\n",
    "    reader = csv.reader(f)\n",
    "    sentences = [''.join(line) for line in reader]\n",
    "sentences = sentences[:10]   \n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([14, 17, 16,  1, 13]),\n",
       " array([14,  3, 16,  5, 13]),\n",
       " array([14,  5,  2, 10, 15,  6, 13]),\n",
       " array([14,  7, 16,  4, 13]),\n",
       " array([14,  8, 16, 18, 13]),\n",
       " array([14, 18,  2, 15, 13]),\n",
       " array([14,  9,  2, 10, 11, 13]),\n",
       " array([14,  0,  2, 10, 12, 13])]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN = numpyNN(sentences, 10,2)\n",
    "NN.data_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start_token', 'sajeeva', 'like', 'hockey', 'end_token']"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = np.array(NN.data_tokens)\n",
    "sen_o = [NN.get_key(v) for v in xx[0]]\n",
    "sen_o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = [\"%s %s %s\" %(start_token, x.lower(), end_token) for x in sentences]\n",
    "sentences = [ (sent.replace(' ',',')) for sent in sentences]\n",
    "sentences = [ sent.split(',') for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['start_token', 'sajeeva', 'like', 'hockey', 'SENTENCE_END'],\n",
       " ['start_token', 'shihan', 'like', 'cricket', 'SENTENCE_END'],\n",
       " ['start_token', 'cricket', 'is', 'a', 'fun', 'game', 'SENTENCE_END'],\n",
       " ['start_token', 'nobody', 'like', 'curling', 'SENTENCE_END'],\n",
       " ['start_token', 'dihan', 'like', 'jumping', 'SENTENCE_END'],\n",
       " ['start_token', 'jumping', 'is', 'fun', 'SENTENCE_END'],\n",
       " ['start_token', 'hansie', 'is', 'a', 'artist', 'SENTENCE_END'],\n",
       " ['start_token', 'dylan', 'is', 'a', 'gamer', 'SENTENCE_END']]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[14, 17, 16, 1, 8],\n",
       " [14, 3, 16, 5, 8],\n",
       " [14, 5, 2, 10, 15, 6, 8],\n",
       " [14, 7, 16, 4, 8],\n",
       " [14, 9, 16, 18, 8],\n",
       " [14, 18, 2, 15, 8],\n",
       " [14, 11, 2, 10, 12, 8],\n",
       " [14, 0, 2, 10, 13, 8]]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = [w for sent in sentences for w in sent]\n",
    "u_words = set(all_words)\n",
    "word_dict = dict([(w,i) for i,w in enumerate(u_words)])\n",
    "sentences_t = []\n",
    "for sent in sentences:\n",
    "    sentences_t.append([word_dict[w] for w in sent])\n",
    "sentences_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 17, 16, 1, 9],\n",
       " [2, 3, 16, 6, 9],\n",
       " [2, 6, 4, 11, 15, 7, 9],\n",
       " [2, 8, 16, 5, 9],\n",
       " [2, 10, 16, 18, 9],\n",
       " [2, 18, 4, 15, 9],\n",
       " [2, 12, 4, 11, 13, 9],\n",
       " [2, 0, 4, 11, 14, 9]]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_w = []\n",
    "for sent in sentences_t:\n",
    "    sentences_w.append([get_key(word_dict,w) for w in sent])\n",
    "sentences_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fun']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_key(word_dict,15)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
